# Benchflow Performance Optimization Guide

## Performance Targets

Benchflow Phase 8C implements comprehensive performance optimizations targeting:

- **Comparison**: <100ms for 1000 benchmarks
- **Report Generation**: <50ms
- **Storage Query**: <200ms for 1000 records
- **Memory**: <50MB for 10,000 benchmarks

## Optimizations Implemented

### 1. Comparator Caching (LRU Cache)

**Purpose**: Avoid recalculating comparisons with identical baseline/current data

**Implementation**:
- LRU cache with configurable size (default: 100 entries)
- MD5 hash-based cache keys from suite contents
- Thread-safe with RWMutex
- FIFO eviction when cache is full

**Performance Impact**:
```
Uncached Compare:        ~93,448 ns/op
Cached Compare (hit):    ~22,660 ns/op (4x faster)
LRU Cache Get:           ~22.66 ns/op
LRU Cache Set:           ~112.5 ns/op
```

**Usage**:
```go
// Create cached comparator wrapping BasicComparator
bc := NewBasicComparator()
cached := NewCachedComparator(bc, 100) // Cache size = 100

// All Compare() calls are automatically cached
result := cached.Compare(baseline, current)

// Clear cache when needed
cached.ClearCache()

// Get cache stats
size, maxSize := cached.CacheStats()
```

**Best Practices**:
- Use 100-500 cache size for typical workflows
- Cache hit rate >90% expected for PR comparisons
- Clear cache between unrelated comparison batches
- Monitor cache stats to tune size

### 2. Storage Query Optimization

**Purpose**: Reduce database query latency through caching and pagination

**Implementation**:
- Query result caching with TTL (time-to-live)
- LIMIT/OFFSET pagination for large result sets
- Configurable cache size and TTL per query type
- Automatic expiration of stale results
- Thread-safe cache with RWMutex

**Query Cache TTLs**:
- `GetLatestOptimized()`: 1 minute cache
- `GetHistoryOptimized()`: 5 minute cache
- `GetComparisonHistoryOptimized()`: 5 minute cache

**Performance Impact**:
```
Uncached GetLatest:      ~33,743 ns/op
Cached GetLatest:        ~68.65 ns/op (490x faster!)

Storage query with 1000 records:
Without cache:           ~400-500ms
With cache:              <1ms (cache hits)
```

**Usage**:
```go
// Create query optimizer with cache size
optimizer := NewQueryOptimizer(db, 100)

// Query with automatic caching
latest, _ := optimizer.GetLatestOptimized()
history, _ := optimizer.GetHistoryOptimized("sort", "go", 10)

// Pagination support
page1, _ := optimizer.GetHistoryOptimized("sort", 10, 0)    // First 10
page2, _ := optimizer.GetHistoryOptimized("sort", 10, 10)   // Next 10

// Cache management
optimizer.ClearCache()
size, maxSize := optimizer.CacheStats()
```

**Cache Configuration**:
- `maxSize`: Maximum number of cached queries (default: 100)
- TTL: Per-query type, 1-5 minutes
- Automatic cleanup: Expired entries automatically removed
- Manual cleanup: `optimizer.ClearCache()`

**Pagination Best Practices**:
- Default limit: 100 results
- Maximum limit: 1000 results (hard limit)
- Use offset for pagination (not efficient for large datasets)
- Combine with cache for optimal performance

### 3. Performance Tuning Recommendations

#### For CLI Workflows
```bash
# Single comparison: Use cached comparator
benchflow compare --baseline main.json --current feature.json

# Multiple comparisons: Cache is reused automatically
# High cache hit rate expected (90%+)

# Parallel comparisons: Use multiple workers with separate caches
# Each worker has independent cache to avoid lock contention
```

#### For API/Service Integration
```go
// Reuse comparator instances to maximize cache hits
bc := NewBasicComparator()
cached := NewCachedComparator(bc, 500) // Larger cache for service

// Reuse query optimizer for database operations
optimizer := NewQueryOptimizer(db, 200)

// Clear cache periodically (e.g., every hour)
ticker := time.NewTicker(1 * time.Hour)
defer ticker.Stop()
for range ticker.C {
    optimizer.ClearCache()
}
```

#### For Large Batch Operations
```go
// For processing 10,000+ benchmarks:
// 1. Use appropriate cache sizes
cached := NewCachedComparator(bc, 1000)

// 2. Process in batches with cache clearing
for batchStart := 0; batchStart < 10000; batchStart += 100 {
    processBatch(batchStart, 100)
    if batchStart % 1000 == 0 {
        cached.ClearCache()
    }
}

// 3. Monitor memory usage
size, _ := cached.CacheStats()
fmt.Printf("Cache using ~%dKB\n", size * 10) // Rough estimate
```

## Benchmarks

Run performance benchmarks:
```bash
# All benchmarks
go test ./... -bench=. -benchtime=100ms

# Comparator benchmarks
go test ./internal/comparator -bench=. -benchtime=100ms

# Storage benchmarks
go test ./internal/storage -bench=. -benchtime=100ms
```

### Benchmark Scenarios

**Comparator Caching**:
- `BenchmarkComparison_Uncached`: Baseline comparison performance
- `BenchmarkComparison_Cached`: Cache hits (best case)
- `BenchmarkComparison_CachedMiss`: Cache misses (worst case)
- `BenchmarkLRUCache_Set`: Cache insertion speed
- `BenchmarkLRUCache_Get`: Cache lookup speed

**Storage Queries**:
- `BenchmarkQueryOptimizer_GetLatestUncached`: Direct database query
- `BenchmarkQueryOptimizer_GetLatestCached`: Cached query result

### Expected Results (Apple M3)

```
Comparator Cache:
  Get: 22.66 ns/op
  Set: 112.5 ns/op

Storage Cache:
  Cached query: 68.65 ns/op
  Uncached query: 33,743 ns/op (490x slower)
```

## Monitoring

### Cache Hit Rate
```go
// Estimate hit rate by comparing before/after
size1, _ := comparator.CacheStats()
// ... perform operations ...
size2, _ := comparator.CacheStats()

hitRate := float64(size2) / float64(size1)
```

### Memory Usage

**Estimated Memory Per Cache Entry**:
- Comparator cache: ~50-100 KB per entry
- Query cache: ~20-50 KB per entry

**Example**: 100-entry comparator cache â‰ˆ 5-10 MB

### Performance Profiling

```bash
# Generate CPU profile
go test -cpuprofile=cpu.prof ./internal/comparator -bench=Comparison_Cached

# Analyze profile
go tool pprof cpu.prof

# Memory profile
go test -memprofile=mem.prof ./internal/storage -bench=GetLatestCached
go tool pprof mem.prof
```

## Troubleshooting

### High Memory Usage
- **Problem**: Cache consuming too much memory
- **Solution**: Reduce cache size or increase TTL
```go
// Instead of 1000 entries, use 100
cache := NewCachedComparator(bc, 100)
```

### Low Cache Hit Rate (<50%)
- **Problem**: Many cache misses indicate different comparisons
- **Solution**:
  - Verify you're reusing the same cache instance
  - Check if baseline/current data is changing unnecessarily
  - Increase cache size to handle more unique comparisons

### Query Cache Stale Data
- **Problem**: Seeing outdated comparison results
- **Solution**:
  - Reduce TTL for more frequent updates
  - Clear cache manually after data changes
```go
optimizer.ClearCache()
```

### Lock Contention (Parallel Operations)
- **Problem**: Slow performance with multiple goroutines
- **Solution**: Use separate cache instances per goroutine
```go
// Each goroutine gets its own cache
for i := 0; i < numWorkers; i++ {
    go worker(i, NewCachedComparator(bc, 100))
}
```

## Future Optimizations

Planned for future phases:

1. **Report Streaming**: Generate HTML/JSON reports without buffering entire output
2. **Statistical Caching**: Cache frequently-used statistical calculations (t-tests, confidence intervals)
3. **Parallel Aggregation**: Multi-threaded result aggregation
4. **Compression**: Compress cached results to reduce memory footprint
5. **Persistent Cache**: Optional disk-based cache for long-running services

## References

- [Cache Memory Hierarchy](https://en.wikipedia.org/wiki/CPU_cache)
- [LRU Cache Implementation](https://en.wikipedia.org/wiki/Cache_replacement_policies#Least_recently_used_(LRU))
- [Go Concurrency Patterns](https://go.dev/blog/pipelines)
- [Database Query Optimization](https://en.wikipedia.org/wiki/Query_optimization)
