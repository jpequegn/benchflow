# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

Benchflow is a cross-language benchmark aggregator with parallel execution and visualization. It's built in Go to leverage goroutines for concurrent benchmark runs across multiple languages (Rust, Python, Go, Node.js).

**Current Status**: All phases complete including Phase 8 enhancements! ðŸš€ Core: Foundation (Phase 1), Rust parser (Phase 2), parallel execution engine (Phase 3), aggregation & storage (Phase 4), HTML reports (Phase 5), multi-language (Phase 6), Node.js (Phase 7). Enhancements: CLI compare command (Phase 8C), historical tracking & trend analysis (Phase 8D), performance optimization & caching (Phase 8C), documentation (Phase 8E). Production ready with 4 language parsers, comparative analysis, trend detection, 490x faster queries (cached), and comprehensive test coverage.

## Development Commands

### Build & Run
```bash
# Build the CLI
go build -o benchflow ./cmd/benchflow

# Run with go run (during development)
go run ./cmd/benchflow [args]

# Install locally
go install ./cmd/benchflow
```

### Testing
```bash
# Run all tests
go test ./...

# Run tests with coverage
go test -cover ./...

# Run specific package tests
go test ./internal/parser
go test ./internal/executor

# Run single test
go test ./internal/parser -run TestParseBencher

# Verbose output
go test -v ./...
```

### Code Quality
```bash
# Format code
go fmt ./...

# Lint (requires golangci-lint)
golangci-lint run

# Vet
go vet ./...

# Run all quality checks
go fmt ./... && go vet ./... && go test ./...
```

## Architecture

Benchflow follows Go's standard project layout with clear separation of concerns:

### Core Pipeline Flow
```
User Config (YAML)
    â†“
CLI (cmd/benchflow) - Entry point, command routing
    â†“
Executor (internal/executor) - Concurrent benchmark execution via goroutines
    â†“
Parser (internal/parser) - Language-specific output parsing
    â†“
Aggregator (internal/aggregator) - Normalize results, calculate statistics
    â†“
Storage (internal/storage) - Historical tracking (SQLite)
    â†“
Reporter (internal/reporter) - Generate HTML/JSON/CSV output
```

### Package Responsibilities

**`cmd/benchflow/`** - CLI entry point
- Framework: cobra or urfave-cli (TBD in Phase 1)
- Commands: `run`, `compare`, `report`
- Configuration loading via viper (YAML/TOML)

**`internal/parser/`** - Multi-language benchmark parsers
- Rust: cargo bench bencher format + criterion
- Python: pytest-benchmark JSON
- Go: testing.B output
- Interface-based design for extensibility
- See Phase 2 issue for Rust parser details

**`internal/executor/`** - Concurrent execution engine
- Worker pool pattern with goroutines
- Context-based timeout/cancellation
- Process spawning via os/exec
- Real-time output streaming to parser
- Error handling and retry logic
- Reference: https://github.com/jpequegn/parakeet-podcast-processor parallel transcription

**`internal/aggregator/`** - Result normalization
- Unified result format across languages
- Statistical calculations (mean, median, std dev)
- Comparison logic for baseline vs current
- Regression detection with configurable thresholds

**`internal/storage/`** - Historical tracking
- SQLite for persistence
- Time-series benchmark data
- Query interface for trend analysis

**`internal/reporter/`** - Output generation
- HTML reports with html/template + Chart.js
- Dark mode support (Nebula UI style)
- Self-contained output (embedded CSS/JS via embed package)
- JSON/CSV export for CI/CD integration

**`pkg/benchflow/`** - Public API
- Stable interfaces for external consumption
- Versioned according to semantic versioning

**`testdata/`** - Test fixtures
- Sample benchmark outputs for each language
- Edge case data for parser validation

**`examples/`** - Configuration examples
- Sample benchflow.yaml files
- Multi-language benchmark setups

## Implementation Phases

All phases complete (tracked in GitHub Issues):

### Core Phases (1-7)
1. **Foundation** (#1) - âœ… COMPLETE - CLI framework, config, logging, tests, CI/CD
2. **Rust Parser** (#2) - âœ… COMPLETE - Bencher/criterion format parsing (82.9% coverage)
3. **Execution Engine** (#3) - âœ… COMPLETE - Goroutine-based parallel execution (94.0% coverage)
4. **Aggregation & Storage** (#4) - âœ… COMPLETE - Statistical aggregation, JSON/CSV export, SQLite storage, regression detection (94.0% aggregator, 82.2% storage coverage)
5. **HTML Reports** (#5) - âœ… COMPLETE - Interactive HTML reports with Chart.js, Nebula UI dark theme, responsive design (75.6% coverage)
6. **Multi-language** (#6) - âœ… COMPLETE - Python pytest-benchmark (#11) and Go testing.B (#12) parsers
7. **Node.js Parser** (#15) - âœ… COMPLETE - Benchmark.js text format parser (81.2% coverage)

### Enhancement Phase (8)
8. **Advanced Features** - âœ… COMPLETE
   - **8C: CLI Integration** - `benchflow compare` command with JSON/CSV loading, regression detection
   - **8C: Performance Optimization** - LRU caching (4x faster comparisons), query optimization (490x faster storage queries)
   - **8D: Historical Tracking** - Trend analysis with linear regression, anomaly detection (Z-score), performance forecasting
   - **8E: Documentation** - Comprehensive guides (COMPARISON.md, API_COMPARATOR.md, STATISTICS.md, PERFORMANCE.md)

**Current Status**: Production ready! âœ… All core and enhancement functionality implemented and tested. 4 language parsers (Rust, Python, Go, Node.js). Comparative analysis engine. Trend detection and forecasting. 490x faster cached queries. 94%+ test coverage on core packages. Ready for advanced use cases and performance-critical deployments.

## Key Design Patterns

### Interface-Based Parsers
Each language parser implements a common interface:
```go
type Parser interface {
    Parse(output io.Reader) (*BenchmarkResult, error)
}
```

### Worker Pool Execution
Executor uses goroutines + channels for concurrent benchmark runs:
- Worker pool with configurable concurrency
- Context for timeout/cancellation propagation
- Channel-based result collection

### Unified Result Format
All parsers normalize to a common struct for aggregation:
```go
type BenchmarkResult struct {
    Name       string
    Language   string
    Time       Duration
    Iterations int
    StdDev     Duration
    // ... additional metrics
}
```

## Configuration

Benchflow uses YAML configuration files (via viper):

```yaml
benchmarks:
  - name: "rust-sort"
    language: rust
    command: "cargo bench --bench sort"
    timeout: 5m

  - name: "python-search"
    language: python
    command: "pytest --benchmark-only"
    timeout: 3m

output:
  formats: [html, json, csv]
  directory: ./reports

storage:
  enabled: true
  path: ./benchflow.db
```

## Testing Strategy

- **Unit tests**: All parsers, aggregators, reporters (80%+ coverage goal)
- **Integration tests**: Full pipeline with testdata fixtures
- **Table-driven tests**: Go idiom for multiple test cases
- **Golden files**: Expected output comparisons for reporters
- **CI/CD**: GitHub Actions runs tests on every PR

## CLI Usage

```bash
# Run all benchmarks from config (âœ… IMPLEMENTED)
benchflow run --config benchflow.yaml

# Run with custom parallelism (âœ… IMPLEMENTED)
benchflow run --parallel 8

# Run specific benchmark (âœ… IMPLEMENTED)
benchflow run --name rust-sort-algorithms

# Run with timeout (âœ… IMPLEMENTED)
benchflow run --timeout 10m

# Compare against baseline (Phase 4)
benchflow compare --baseline v1.0.0 --current HEAD

# Generate HTML report (Phase 5)
benchflow report --format html --output report.html
```

### Implemented Features (Phase 3)

The executor supports:
- **Parallel execution**: Worker pool pattern with configurable concurrency (default: 4)
- **Context-based cancellation**: Graceful shutdown on CTRL+C or timeout
- **Automatic retry**: Configurable retry attempts with exponential backoff
- **Progress tracking**: Real-time event notifications (started, completed, failed, retrying)
- **Fail-fast mode**: Stop on first failure (optional)
- **Parser registry**: Plugin architecture for language-specific parsers
- **Comprehensive error handling**: Execution errors, parsing errors, timeouts
- **Performance**: 10 benchmarks with 5 workers complete in ~240ms vs 1s sequential

Test coverage: 94.0%

## Related Projects

- **Parakeet Podcast Processor**: Reference for parallel execution patterns
  - Path: `/Users/julienpequegnot/Code/parakeet-podcast-processor`
  - See: `p3 transcribe` for goroutine-based concurrent processing
  - Similar worker pool + error handling approach

## Dependencies (Planned)

- **CLI**: cobra or urfave-cli
- **Config**: spf13/viper
- **Logging**: log/slog (Go 1.21+ stdlib)
- **Database**: mattn/go-sqlite3
- **Testing**: stretchr/testify (assertions)
- **Web**: stdlib html/template, Chart.js (embedded)
